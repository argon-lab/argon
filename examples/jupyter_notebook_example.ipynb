{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Argon Jupyter Integration Example\n",
    "\n",
    "This notebook demonstrates how to use Argon's Jupyter integration to track experiments, manage data, and create reproducible ML workflows.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's load the Argon extension and initialize our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Argon extension\n",
    "%load_ext argon.integrations.jupyter_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Argon for this notebook\n",
    "%argon_init ml-notebook-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set our working branch\n",
    "%argon_branch random-forest-experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Let's create some sample data and save it to our Argon branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%argon_track\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.datasets import make_classification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create sample data\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=20,\n",
    "    n_informative=10,\n",
    "    n_redundant=5,\n",
    "    n_clusters_per_class=1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for easier handling\n",
    "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Target distribution: {df['target'].value_counts().to_dict()}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataset to our Argon branch\n",
    "from argon.integrations.jupyter import get_argon_integration\n",
    "\n",
    "integration = get_argon_integration()\n",
    "dataset_path = integration.save_dataset(\n",
    "    df, \n",
    "    \"classification_dataset\", \n",
    "    \"Synthetic classification dataset with 1000 samples and 20 features\"\n",
    ")\n",
    "\n",
    "print(f\"Dataset saved to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration\n",
    "\n",
    "Let's set up our experiment parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log experiment parameters\n",
    "%argon_params n_estimators=100 max_depth=10 random_state=42 test_size=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint after data preparation\n",
    "%argon_checkpoint data_prepared --description \"Dataset created and parameters set\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "Now let's train our Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%argon_track\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df.drop('target', axis=1), \n",
    "    df['target'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%argon_track\n",
    "# Train the model\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Model training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = integration.save_model(\n",
    "    model, \n",
    "    \"random_forest_v1\", \n",
    "    \"Random Forest classifier with 100 estimators\",\n",
    "    metadata={\n",
    "        \"algorithm\": \"RandomForest\",\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"training_samples\": len(X_train)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Model saved to: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "Let's evaluate our model and log the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%argon_track\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics to Argon\n",
    "%argon_metrics accuracy=0.95 precision=0.94 recall=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a checkpoint after model training\n",
    "%argon_checkpoint model_trained --description \"Random Forest model trained and evaluated\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "Let's create some visualizations to understand our model better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%argon_track\n",
    "# Plot feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(10), feature_importance['importance'][:10])\n",
    "plt.yticks(range(10), feature_importance['feature'][:10])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Save feature importance data\n",
    "feature_importance_path = integration.save_dataset(\n",
    "    feature_importance, \n",
    "    \"feature_importance\", \n",
    "    \"Feature importance scores from Random Forest model\"\n",
    ")\n",
    "print(f\"Feature importance saved to: {feature_importance_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning Experiment\n",
    "\n",
    "Let's create a new branch for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new branch for hyperparameter tuning\n",
    "%argon_branch hyperparameter_tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set different parameters for this experiment\n",
    "%argon_params n_estimators=200 max_depth=15 random_state=42 test_size=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%argon_track\n",
    "# Load the dataset from the previous branch\n",
    "df_loaded = integration.load_dataset(\"classification_dataset\")\n",
    "\n",
    "# Train model with different parameters\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(\n",
    "    df_loaded.drop('target', axis=1), \n",
    "    df_loaded['target'], \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_2 = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model_2.fit(X_train_2, y_train_2)\n",
    "y_pred_2 = model_2.predict(X_test_2)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy_2 = accuracy_score(y_test_2, y_pred_2)\n",
    "precision_2 = precision_score(y_test_2, y_pred_2, average='weighted')\n",
    "recall_2 = recall_score(y_test_2, y_pred_2, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy_2:.4f}\")\n",
    "print(f\"Precision: {precision_2:.4f}\")\n",
    "print(f\"Recall: {recall_2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics for the tuned model\n",
    "%argon_metrics accuracy=0.96 precision=0.95 recall=0.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the tuned model\n",
    "model_2_path = integration.save_model(\n",
    "    model_2, \n",
    "    \"random_forest_v2_tuned\", \n",
    "    \"Random Forest classifier with 200 estimators - hyperparameter tuned\",\n",
    "    metadata={\n",
    "        \"algorithm\": \"RandomForest\",\n",
    "        \"framework\": \"scikit-learn\",\n",
    "        \"training_samples\": len(X_train_2),\n",
    "        \"tuned\": True\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Tuned model saved to: {model_2_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Comparison\n",
    "\n",
    "Let's compare our two experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare experiments across branches\n",
    "%argon_compare random-forest-experiment hyperparameter_tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Status and Export\n",
    "\n",
    "Let's check our current status and export our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current status\n",
    "%argon_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export notebook results\n",
    "integration.export_notebook_results(\"notebook_results.json\")\n",
    "print(\"Notebook results exported to notebook_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "\n",
    "1. **Initialization**: Setting up Argon for notebook use\n",
    "2. **Branch Management**: Creating and switching between experiment branches\n",
    "3. **Data Tracking**: Saving and loading datasets with version control\n",
    "4. **Parameter Logging**: Tracking experiment parameters\n",
    "5. **Model Management**: Saving and loading trained models\n",
    "6. **Metrics Tracking**: Logging and comparing experiment metrics\n",
    "7. **Checkpoints**: Creating snapshots of experiment state\n",
    "8. **Cell Tracking**: Monitoring individual cell executions\n",
    "9. **Experiment Comparison**: Comparing results across different branches\n",
    "10. **Export**: Exporting experiment results for analysis\n",
    "\n",
    "### Key Benefits:\n",
    "- **Reproducibility**: All experiments are tracked and can be reproduced\n",
    "- **Version Control**: Data and models are versioned alongside code\n",
    "- **Collaboration**: Teams can share and compare experiments easily\n",
    "- **Organization**: Experiments are organized in logical branches\n",
    "- **Integration**: Seamless integration with existing ML workflows\n",
    "\n",
    "### Next Steps:\n",
    "- Try integrating with MLflow, DVC, or W&B for additional tracking\n",
    "- Experiment with different models and parameters\n",
    "- Share your experiments with team members\n",
    "- Use the exported data for further analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}