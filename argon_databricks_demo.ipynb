{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca25456b",
   "metadata": {},
   "source": [
    "# Argon: Local Branching & S3 Demo\n",
    "\n",
    "This notebook demonstrates Argon's core features: MongoDB branching, stateless compute, and S3 storage. All operations are local (no Databricks/Spark required)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e604f6",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration\n",
    "\n",
    "Install required libraries and configure Argon and S3 access. Ensure Docker is running and your `.env` is set up with AWS credentials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e6b11",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.13.3' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# Install dependencies (run in terminal if not already installed)\n",
    "# !pip install boto3 python-dotenv docker\n",
    "\n",
    "import os\n",
    "from core import branch_manager, metadata\n",
    "\n",
    "# Initialize metadata DB\n",
    "metadata.init_db()\n",
    "\n",
    "# S3 and Docker are configured via .env and Docker Desktop\n",
    "print('Setup complete. Ready for Argon demo!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae568ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e4e50aaf",
   "metadata": {},
   "source": [
    "## 2. Create, List, and Time-Travel Branches\n",
    "\n",
    "Create a new MongoDB branch from a base snapshot (from S3), list all branches, and demonstrate point-in-time restore (time travel) to a previous state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f48d136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Argon\n",
    "argon_df = spark.read.format(\"argon\").option(\"uri\", argon_uri).load()\n",
    "argon_df.show()\n",
    "\n",
    "# Write data to Argon\n",
    "# argon_df.write.format(\"argon\").option(\"uri\", argon_uri).mode(\"append\").save()\n",
    "\n",
    "# Create a new branch (from base S3 snapshot)\n",
    "branch = branch_manager.create_branch('demo-branch')\n",
    "print('Created branch:', branch)\n",
    "\n",
    "# List all branches\n",
    "branches = branch_manager.list_branches()\n",
    "print('All branches:', branches)\n",
    "\n",
    "# Simulate some changes and delete the branch to create a new S3 version\n",
    "# (In a real demo, you would modify data in the running MongoDB container here)\n",
    "result = branch_manager.delete_branch('demo-branch')\n",
    "print('Branch deleted:', result)\n",
    "\n",
    "# List all branches again (should be empty)\n",
    "branches = branch_manager.list_branches()\n",
    "print('All branches after delete:', branches)\n",
    "\n",
    "# Time-travel: restore the branch to its previous state using the CLI\n",
    "# Example CLI command (run in terminal):\n",
    "# python3 -m cli.main branch time-travel demo-branch --from demo-branch --timestamp \"2025-05-27T12:00:00Z\"\n",
    "\n",
    "# Or use the Python API directly:\n",
    "from core.metadata import get_branch_version_by_time\n",
    "vinfo = get_branch_version_by_time('demo-branch', '2025-05-27T12:00:00Z')\n",
    "if vinfo:\n",
    "    print('Found version for time-travel:', vinfo)\n",
    "    branch = branch_manager.create_branch('demo-branch', vinfo['s3_path'], vinfo['version_id'])\n",
    "    print('Restored branch from time-travel:', branch)\n",
    "else:\n",
    "    print('No version found for time-travel at the given timestamp.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb1e5dd",
   "metadata": {},
   "source": [
    "## 3. Use a Branch for Local Development\n",
    "\n",
    "Connect to the MongoDB branch (container) on its assigned port for development, testing, or CI/CD workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e9029c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new branch for an experiment (pseudo-code)\n",
    "import requests\n",
    "from pymongo import MongoClient\n",
    "\n",
    "branch_name = \"experiment-2025-05-27\"\n",
    "# requests.post(f\"{argon_uri}/branches\", json={\"name\": branch_name, \"from\": \"main\"})\n",
    "\n",
    "# Use the branch for ML training\n",
    "experiment_uri = f\"argon://<your-argon-endpoint>/{branch_name}\"\n",
    "experiment_df = spark.read.format(\"argon\").option(\"uri\", experiment_uri).load()\n",
    "\n",
    "# Train a model on the experiment branch\n",
    "def train_model(df):\n",
    "    # ... ML training code ...\n",
    "    pass\n",
    "\n",
    "train_model(experiment_df)\n",
    "\n",
    "# Record branch name in MLflow for reproducibility\n",
    "# mlflow.log_param(\"argon_branch\", branch_name)\n",
    "\n",
    "# Example: Connect to the running MongoDB branch\n",
    "branch_port = branch['port']\n",
    "client = MongoClient(f'mongodb://localhost:{branch_port}/')\n",
    "db = client['test']\n",
    "print('Collections:', db.list_collection_names())\n",
    "\n",
    "# You can now use this branch for any local dev/test/CI workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08f3e85",
   "metadata": {},
   "source": [
    "## 4. Delete a Branch and Persist to S3\n",
    "\n",
    "When finished, delete the branch. Argon will dump the data to S3 and remove the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f087f891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Store features in Argon\n",
    "features = [\n",
    "    {\"user_id\": 1, \"features\": {\"age\": 25, \"country\": \"US\", \"history\": [1, 2, 3]}},\n",
    "    {\"user_id\": 2, \"features\": {\"age\": 32, \"country\": \"UK\", \"history\": [2, 4, 6]}}\n",
    "]\n",
    "\n",
    "features_df = spark.createDataFrame(features)\n",
    "features_df.write.format(\"argon\").option(\"uri\", argon_uri).mode(\"append\").save()\n",
    "\n",
    "# Retrieve features for ML\n",
    "retrieved_df = spark.read.format(\"argon\").option(\"uri\", argon_uri).load()\n",
    "retrieved_df.show()\n",
    "\n",
    "# Delete the branch (dump to S3, remove container)\n",
    "result = branch_manager.delete_branch('demo-branch')\n",
    "print('Branch deleted:', result)\n",
    "\n",
    "# List branches again to confirm\n",
    "git_branches = branch_manager.list_branches()\n",
    "print('Remaining branches:', git_branches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919a6d9",
   "metadata": {},
   "source": [
    "## 5. Summary: Argon Local Demo Value\n",
    "\n",
    "- Instantly create isolated MongoDB branches from S3 snapshots\n",
    "- Use branches for local dev, testing, or CI/CD\n",
    "- Persist branch data to S3 for stateless compute\n",
    "- No Databricks or Spark required for this demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54f413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store inference results\n",
    "inference_results = [\n",
    "    {\"user_id\": 1, \"prediction\": 0.87, \"timestamp\": \"2025-05-27T10:00:00Z\"},\n",
    "    {\"user_id\": 2, \"prediction\": 0.42, \"timestamp\": \"2025-05-27T10:01:00Z\"}\n",
    "]\n",
    "\n",
    "inference_df = spark.createDataFrame(inference_results)\n",
    "inference_df.write.format(\"argon\").option(\"uri\", argon_uri).mode(\"append\").save()\n",
    "\n",
    "# Retrieve feedback for retraining\n",
    "feedback_df = spark.read.format(\"argon\").option(\"uri\", argon_uri).load()\n",
    "feedback_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e17c0",
   "metadata": {},
   "source": [
    "## 6. Model Metadata and Artifact Storage\n",
    "\n",
    "Use Argon to store model metadata, inputs, outputs, and evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store model metadata\n",
    "model_metadata = {\n",
    "    \"model_name\": \"churn-predictor-v1\",\n",
    "    \"inputs\": [\"age\", \"country\", \"history\"],\n",
    "    \"outputs\": [\"churn_probability\"],\n",
    "    \"metrics\": {\"auc\": 0.91, \"accuracy\": 0.88},\n",
    "    \"trained_on_branch\": branch_name,\n",
    "    \"timestamp\": \"2025-05-27T10:05:00Z\"\n",
    "}\n",
    "\n",
    "metadata_df = spark.createDataFrame([model_metadata])\n",
    "metadata_df.write.format(\"argon\").option(\"uri\", argon_uri).mode(\"append\").save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b3cce",
   "metadata": {},
   "source": [
    "## 7. Governance Integration with Unity Catalog\n",
    "\n",
    "Argon can expose its schema and access controls through a Unity Catalog-compatible interface, enabling unified governance, data lineage, and security in Databricks environments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
